Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: mgrakotomanga (mgrakotomanga-university-of-leeds) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run hygwnx32
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /gws/ssde/j25a/mmh_storage/train110/wandb_logs/wandb/run-20251112_185118-hygwnx32
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unet3d_himawari_ddp
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mgrakotomanga-university-of-leeds/pyearthtools-nowcasting
wandb: üöÄ View run at https://wandb.ai/mgrakotomanga-university-of-leeds/pyearthtools-nowcasting/runs/hygwnx32
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name      | Type          | Params | Mode 
----------------------------------------------------
0 | model     | UNet3D        | 5.4 M  | train
1 | criterion | PooledMSELoss | 0      | train
----------------------------------------------------
5.4 M     Trainable params
0         Non-trainable params
5.4 M     Total params
21.733    Total estimated model params size (MB)
59        Modules in train mode
0         Modules in eval mode
slurmstepd: error: *** JOB 49726478 ON gpuhost011 CANCELLED AT 2025-11-12T18:52:32 ***
