Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: mgrakotomanga (mgrakotomanga-university-of-leeds) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run evcmvzsw
wandb: Tracking run with wandb version 0.23.0
wandb: Run data is saved locally in /gws/ssde/j25a/mmh_storage/train110/wandb_logs/wandb/run-20251112_185522-evcmvzsw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unet3d_himawari_ddp
wandb: ‚≠êÔ∏è View project at https://wandb.ai/mgrakotomanga-university-of-leeds/pyearthtools-nowcasting
wandb: üöÄ View run at https://wandb.ai/mgrakotomanga-university-of-leeds/pyearthtools-nowcasting/runs/evcmvzsw
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

  | Name      | Type          | Params | Mode 
----------------------------------------------------
0 | model     | UNet3D        | 5.4 M  | train
1 | criterion | PooledMSELoss | 0      | train
----------------------------------------------------
5.4 M     Trainable params
0         Non-trainable params
5.4 M     Total params
21.733    Total estimated model params size (MB)
59        Modules in train mode
0         Modules in eval mode
double free or corruption (out)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1275, in _try_get_data
[rank3]:     data = self._data_queue.get(timeout=timeout)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/apps/jasmin/jaspy/miniforge_envs/jaspy3.12/mf3-25.3.0-3/lib/python3.12/queue.py", line 180, in get
[rank3]:     self.not_empty.wait(remaining)
[rank3]:   File "/apps/jasmin/jaspy/miniforge_envs/jaspy3.12/mf3-25.3.0-3/lib/python3.12/threading.py", line 359, in wait
[rank3]:     gotit = waiter.acquire(True, timeout)
[rank3]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py", line 73, in handler
[rank3]:     _error_if_any_worker_fails()
[rank3]: RuntimeError: DataLoader worker (pid 1564248) is killed by signal: Aborted. 

[rank3]: The above exception was the direct cause of the following exception:

[rank3]: Traceback (most recent call last):
[rank3]:   File "/home/users/train110/Nowcasting-with-PyEarthTools/script/3D-Unet-DDP.py", line 318, in <module>
[rank3]:     trainer.fit(lit_model, train_loader, val_loader)
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 560, in fit
[rank3]:     call._call_and_handle_interrupt(
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py", line 48, in _call_and_handle_interrupt
[rank3]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 105, in launch
[rank3]:     return function(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 598, in _fit_impl
[rank3]:     self._run(model, ckpt_path=ckpt_path)
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1011, in _run
[rank3]:     results = self._run_stage()
[rank3]:               ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py", line 1055, in _run_stage
[rank3]:     self.fit_loop.run()
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
[rank3]:     self.advance()
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py", line 458, in advance
[rank3]:     self.epoch_loop.run(self._data_fetcher)
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 152, in run
[rank3]:     self.advance(data_fetcher)
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 310, in advance
[rank3]:     batch, _, __ = next(data_fetcher)
[rank3]:                    ^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 134, in __next__
[rank3]:     batch = super().__next__()
[rank3]:             ^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/loops/fetchers.py", line 61, in __next__
[rank3]:     batch = next(self.iterator)
[rank3]:             ^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
[rank3]:     out = next(self._iterator)
[rank3]:           ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
[rank3]:     out[i] = next(self.iterators[i])
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 732, in __next__
[rank3]:     data = self._next_data()
[rank3]:            ^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1482, in _next_data
[rank3]:     idx, data = self._get_data()
[rank3]:                 ^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1434, in _get_data
[rank3]:     success, data = self._try_get_data()
[rank3]:                     ^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/home/users/train110/.local/lib/python3.12/site-packages/torch/utils/data/dataloader.py", line 1288, in _try_get_data
[rank3]:     raise RuntimeError(
[rank3]: RuntimeError: DataLoader worker (pid(s) 1564248) exited unexpectedly
slurmstepd: error: *** JOB 49726927 ON gpuhost010 CANCELLED AT 2025-11-12T19:45:50 ***
